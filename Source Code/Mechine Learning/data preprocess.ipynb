{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eleven-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sensitive_data = pd.read_excel('./sensitive_data.xlsx')\n",
    "non_sensitive_data = pd.read_excel('./non-sensitive_data.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intermediate-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "p = re.compile('^@.*?\\s')\n",
    "#res = re.sub(p,'',train_data['content'])\n",
    "\n",
    "sensitive_data['content'] = sensitive_data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('@')]))\n",
    "sensitive_data['content'] = sensitive_data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('#')]))\n",
    "sensitive_data['content'] = sensitive_data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('http')]))\n",
    "\n",
    "non_sensitive_data['content'] = non_sensitive_data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('@')]))\n",
    "non_sensitive_data['content'] = non_sensitive_data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('#')]))\n",
    "non_sensitive_data['content'] = non_sensitive_data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('http')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southwest-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_data = sensitive_data.replace('[^\\w\\s]','',regex=True)\n",
    "sensitive_data.content = sensitive_data.content.map(lambda x:x.lower())\n",
    "sensitive_data.to_excel('sensitive_train.xlsx')\n",
    "\n",
    "stopwords = []\n",
    "with open('./stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if len(line)>0:\n",
    "            stopwords.append(line.strip())\n",
    "            \n",
    "sensitive_data['without_stopwords'] = sensitive_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "sensitive_data.to_excel('processed_sensitive.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "binding-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sensitive_data = non_sensitive_data.replace('[^\\w\\s]','',regex=True)\n",
    "non_sensitive_data.content = non_sensitive_data.content.map(lambda x:x.lower())\n",
    "non_sensitive_data.to_excel('non-sensitive_train.xlsx')\n",
    "\n",
    "stopwords = []\n",
    "with open('./stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if len(line)>0:\n",
    "            stopwords.append(line.strip())\n",
    "            \n",
    "non_sensitive_data['without_stopwords'] = non_sensitive_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "non_sensitive_data.to_excel('processed_non-sensitive.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "critical-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "sensitive_data['without_stopwords'] = sensitive_data.without_stopwords.apply(lemmatize_text)\n",
    "non_sensitive_data['without_stopwords'] = non_sensitive_data.without_stopwords.apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mighty-ethics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if this colleague of mine talks in japanese on...</td>\n",
       "      <td>[colleague, mine, talk, japanese, punching, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>several hours after the game none of the team ...</td>\n",
       "      <td>[hour, game, team, india, member, spoken, coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yesterday my colleague said something i will n...</td>\n",
       "      <td>[yesterday, colleague, quoting, rest, life, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt  what can i do to make my conspiracy theori...</td>\n",
       "      <td>[conspiracy, theorist, colleague, reptilian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boss is it on youtube cause im in a fuckin  is...</td>\n",
       "      <td>[bos, youtube, fuckin, islamic, republici, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>if i want to change my job a fortune 500 compa...</td>\n",
       "      <td>[change, job, fortune, 500, company, 10, salar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>i cant stand this awful working environment im...</td>\n",
       "      <td>[stand, awful, working, environment, jump, shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>tired do not want to go to work began and long...</td>\n",
       "      <td>[tired, work, began, longterm, job, burnout, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>overtime overtime fuck overtime i dont want ov...</td>\n",
       "      <td>[overtime, overtime, fuck, overtime, dont, ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>the boss never works overtime because the boss...</td>\n",
       "      <td>[bos, work, overtime, bos, work, overtime, shi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  \\\n",
       "0    if this colleague of mine talks in japanese on...   \n",
       "1    several hours after the game none of the team ...   \n",
       "2    yesterday my colleague said something i will n...   \n",
       "3    rt  what can i do to make my conspiracy theori...   \n",
       "4    boss is it on youtube cause im in a fuckin  is...   \n",
       "..                                                 ...   \n",
       "200  if i want to change my job a fortune 500 compa...   \n",
       "201  i cant stand this awful working environment im...   \n",
       "202  tired do not want to go to work began and long...   \n",
       "203  overtime overtime fuck overtime i dont want ov...   \n",
       "204  the boss never works overtime because the boss...   \n",
       "\n",
       "                                     without_stopwords  \n",
       "0    [colleague, mine, talk, japanese, punching, th...  \n",
       "1    [hour, game, team, india, member, spoken, coll...  \n",
       "2    [yesterday, colleague, quoting, rest, life, ph...  \n",
       "3         [conspiracy, theorist, colleague, reptilian]  \n",
       "4    [bos, youtube, fuckin, islamic, republici, hea...  \n",
       "..                                                 ...  \n",
       "200  [change, job, fortune, 500, company, 10, salar...  \n",
       "201  [stand, awful, working, environment, jump, shi...  \n",
       "202  [tired, work, began, longterm, job, burnout, t...  \n",
       "203  [overtime, overtime, fuck, overtime, dont, ove...  \n",
       "204  [bos, work, overtime, bos, work, overtime, shi...  \n",
       "\n",
       "[205 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "consistent-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 300)\t1\n",
      "  (0, 972)\t1\n",
      "  (0, 1456)\t1\n",
      "  (0, 809)\t1\n",
      "  (0, 1169)\t1\n",
      "  (0, 1496)\t1\n",
      "  (0, 1259)\t1\n",
      "  (0, 1600)\t1\n",
      "  (0, 343)\t1\n",
      "  (0, 1637)\t1\n",
      "  (1, 300)\t1\n",
      "  (1, 735)\t1\n",
      "  (1, 635)\t1\n",
      "  (1, 1470)\t1\n",
      "  (1, 770)\t2\n",
      "  (1, 952)\t1\n",
      "  (1, 1380)\t1\n",
      "  (1, 1301)\t1\n",
      "  (1, 1377)\t1\n",
      "  (1, 802)\t1\n",
      "  (1, 706)\t1\n",
      "  (1, 366)\t1\n",
      "  (1, 402)\t1\n",
      "  (1, 1064)\t1\n",
      "  (2, 300)\t1\n",
      "  :\t:\n",
      "  (201, 1327)\t1\n",
      "  (201, 1388)\t1\n",
      "  (201, 1454)\t1\n",
      "  (201, 681)\t1\n",
      "  (201, 178)\t1\n",
      "  (201, 506)\t1\n",
      "  (202, 1649)\t1\n",
      "  (202, 813)\t1\n",
      "  (202, 1486)\t1\n",
      "  (202, 223)\t1\n",
      "  (202, 1503)\t1\n",
      "  (202, 172)\t1\n",
      "  (202, 903)\t1\n",
      "  (202, 201)\t1\n",
      "  (203, 621)\t1\n",
      "  (203, 1084)\t1\n",
      "  (203, 1329)\t1\n",
      "  (203, 457)\t1\n",
      "  (203, 1058)\t4\n",
      "  (204, 199)\t2\n",
      "  (204, 1649)\t2\n",
      "  (204, 1329)\t1\n",
      "  (204, 555)\t1\n",
      "  (204, 1058)\t2\n",
      "  (204, 708)\t1\n",
      "  (0, 0)\t2\n",
      "  (0, 5)\t2\n",
      "  (0, 3)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 0)\t2\n",
      "  (1, 5)\t2\n",
      "  (1, 3)\t2\n",
      "  (1, 4)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 0)\t2\n",
      "  (2, 5)\t2\n",
      "  (2, 3)\t2\n",
      "  (2, 4)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 0)\t2\n",
      "  (3, 5)\t2\n",
      "  (3, 3)\t2\n",
      "  (3, 4)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 0)\t2\n",
      "  :\t:\n",
      "  (290, 1)\t1\n",
      "  (291, 0)\t2\n",
      "  (291, 5)\t2\n",
      "  (291, 3)\t2\n",
      "  (291, 4)\t1\n",
      "  (291, 2)\t1\n",
      "  (291, 1)\t1\n",
      "  (292, 0)\t2\n",
      "  (292, 5)\t2\n",
      "  (292, 3)\t2\n",
      "  (292, 4)\t1\n",
      "  (292, 2)\t1\n",
      "  (292, 1)\t1\n",
      "  (293, 0)\t2\n",
      "  (293, 5)\t2\n",
      "  (293, 3)\t2\n",
      "  (293, 4)\t1\n",
      "  (293, 2)\t1\n",
      "  (293, 1)\t1\n",
      "  (294, 0)\t2\n",
      "  (294, 5)\t2\n",
      "  (294, 3)\t2\n",
      "  (294, 4)\t1\n",
      "  (294, 2)\t1\n",
      "  (294, 1)\t1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5c046b12dd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msensitive_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_sensitive_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"特征词:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sensitive_list = []\n",
    "non_sensitive_list = []\n",
    "\n",
    "sensitive_set = sensitive_data['without_stopwords']\n",
    "for i in sensitive_set:  # 对于双层列表中的数据\n",
    "    i = str(i).strip('[').strip(']').replace(',', '').replace('\\'', '') + '\\n'  # 将其中每一个列表规范化成字符串\n",
    "    sensitive_list.append(i)\n",
    "\n",
    "non_sensitive_set = non_sensitive_data['without_stopwords']\n",
    "for j in non_sensitive_set:  # 对于双层列表中的数据\n",
    "    i = str(i).strip('[').strip(']').replace(',', '').replace('\\'', '') + '\\n'  # 将其中每一个列表规范化成字符串\n",
    "    non_sensitive_list.append(i)\n",
    "    \n",
    "# 实例化分词对象\n",
    "vec = CountVectorizer(min_df=1)\n",
    " \n",
    "# 将文本进行词袋处理\n",
    "X0 = vec.fit_transform(sensitive_list)\n",
    "X1 = vec.fit_transform(non_sensitive_list)\n",
    "print(X0)\n",
    "print(X1)\n",
    "X = vec.fit_transform([sensitive_list, non_sensitive_list])\n",
    "fnames = vec.get_feature_names()\n",
    "print(\"特征词:\", fnames)\n",
    "\n",
    "arr_0 = X0.toarray()\n",
    "print(arr_0)\n",
    "arr_1 = X1.toarray()\n",
    "print(arr_1)\n",
    "\n",
    "print('特征个数:', len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fluid-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bos', 'fairness', 'hehe', 'overtime', 'shit', 'work']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "boring-destiny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[2 1 1 2 1 2]\n",
      " [2 1 1 2 1 2]\n",
      " [2 1 1 2 1 2]\n",
      " ...\n",
      " [2 1 1 2 1 2]\n",
      " [2 1 1 2 1 2]\n",
      " [2 1 1 2 1 2]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "physical-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1637)\t0.3258664522859065\n",
      "  (0, 1600)\t0.3258664522859065\n",
      "  (0, 1496)\t0.3258664522859065\n",
      "  (0, 1456)\t0.2857805114868239\n",
      "  (0, 1259)\t0.3258664522859065\n",
      "  (0, 1169)\t0.3258664522859065\n",
      "  (0, 972)\t0.3258664522859065\n",
      "  (0, 809)\t0.3258664522859065\n",
      "  (0, 343)\t0.3258664522859065\n",
      "  (0, 300)\t0.2623317393132323\n",
      "  (1, 1470)\t0.25738438672215663\n",
      "  (1, 1380)\t0.25738438672215663\n",
      "  (1, 1377)\t0.25738438672215663\n",
      "  (1, 1301)\t0.25738438672215663\n",
      "  (1, 1064)\t0.23886346257378124\n",
      "  (1, 952)\t0.25738438672215663\n",
      "  (1, 802)\t0.25738438672215663\n",
      "  (1, 770)\t0.45144531552849027\n",
      "  (1, 735)\t0.20720173361586983\n",
      "  (1, 706)\t0.25738438672215663\n",
      "  (1, 635)\t0.23886346257378124\n",
      "  (1, 402)\t0.25738438672215663\n",
      "  (1, 366)\t0.25738438672215663\n",
      "  (1, 300)\t0.20720173361586983\n",
      "  (2, 1675)\t0.3923510975994626\n",
      "  :\t:\n",
      "  (201, 514)\t0.24243413343046213\n",
      "  (201, 506)\t0.2764399523360612\n",
      "  (201, 390)\t0.18460928831546822\n",
      "  (201, 311)\t0.18460928831546822\n",
      "  (201, 178)\t0.2764399523360612\n",
      "  (201, 154)\t0.23148670493509796\n",
      "  (202, 1649)\t0.24796401908771046\n",
      "  (202, 1503)\t0.35634385134713914\n",
      "  (202, 1486)\t0.3731960020288779\n",
      "  (202, 903)\t0.4255435633302148\n",
      "  (202, 813)\t0.21157485168092596\n",
      "  (202, 223)\t0.29022708036205697\n",
      "  (202, 201)\t0.4255435633302148\n",
      "  (202, 172)\t0.4255435633302148\n",
      "  (203, 1329)\t0.15435632239865704\n",
      "  (203, 1084)\t0.1967660485143257\n",
      "  (203, 1058)\t0.9392510902150627\n",
      "  (203, 621)\t0.15964520469572557\n",
      "  (203, 457)\t0.1725631653485062\n",
      "  (204, 1649)\t0.42890958251873695\n",
      "  (204, 1329)\t0.22452326227181396\n",
      "  (204, 1058)\t0.68310683874287\n",
      "  (204, 708)\t0.3680366868608804\n",
      "  (204, 555)\t0.21141630041752912\n",
      "  (204, 199)\t0.34473622761087813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "sensitive_tfidf = transformer.fit_transform(X0)\n",
    "non_sensitive_tfidf = transformer.fit_transform(X1)\n",
    "\n",
    "print(sensitive_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "developing-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征个数: 6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "equivalent-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征词: {'bos': 0, 'work': 5, 'overtime': 3, 'shit': 4, 'hehe': 2, 'fairness': 1}\n",
      "词频:   (0, 300)\t1\n",
      "  (0, 343)\t1\n",
      "  (0, 809)\t1\n",
      "  (0, 972)\t1\n",
      "  (0, 1169)\t1\n",
      "  (0, 1259)\t1\n",
      "  (0, 1456)\t1\n",
      "  (0, 1496)\t1\n",
      "  (0, 1600)\t1\n",
      "  (0, 1637)\t1\n",
      "  (1, 300)\t1\n",
      "  (1, 366)\t1\n",
      "  (1, 402)\t1\n",
      "  (1, 635)\t1\n",
      "  (1, 706)\t1\n",
      "  (1, 735)\t1\n",
      "  (1, 770)\t2\n",
      "  (1, 802)\t1\n",
      "  (1, 952)\t1\n",
      "  (1, 1064)\t1\n",
      "  (1, 1301)\t1\n",
      "  (1, 1377)\t1\n",
      "  (1, 1380)\t1\n",
      "  (1, 1470)\t1\n",
      "  (2, 300)\t1\n",
      "  :\t:\n",
      "  (201, 1279)\t1\n",
      "  (201, 1327)\t1\n",
      "  (201, 1388)\t1\n",
      "  (201, 1454)\t1\n",
      "  (201, 1643)\t1\n",
      "  (201, 1652)\t1\n",
      "  (202, 172)\t1\n",
      "  (202, 201)\t1\n",
      "  (202, 223)\t1\n",
      "  (202, 813)\t1\n",
      "  (202, 903)\t1\n",
      "  (202, 1486)\t1\n",
      "  (202, 1503)\t1\n",
      "  (202, 1649)\t1\n",
      "  (203, 457)\t1\n",
      "  (203, 621)\t1\n",
      "  (203, 1058)\t4\n",
      "  (203, 1084)\t1\n",
      "  (203, 1329)\t1\n",
      "  (204, 199)\t2\n",
      "  (204, 555)\t1\n",
      "  (204, 708)\t1\n",
      "  (204, 1058)\t2\n",
      "  (204, 1329)\t1\n",
      "  (204, 1649)\t2\n",
      "词频矩阵 [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('特征词:', vec.vocabulary_)\n",
    "print('词频:', X0)\n",
    "\n",
    "print('词频矩阵', sensitive_tfidf.toarray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-legislation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
