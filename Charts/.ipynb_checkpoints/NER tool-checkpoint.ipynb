{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "preceding-treasury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
      "\u001b[K     |████████████████████████████████| 432 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from stanza) (1.19.2)\n",
      "Requirement already satisfied: requests in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from stanza) (2.25.1)\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-1.10.0-cp38-none-macosx_10_9_x86_64.whl (147.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 147.1 MB 24 kB/s  eta 0:00:01   |█▍                              | 6.6 MB 223 kB/s eta 0:10:28     |██████████████████████████████▊ | 141.1 MB 459 kB/s eta 0:00:14\n",
      "\u001b[?25hCollecting protobuf\n",
      "  Downloading protobuf-3.19.1-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 740 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from stanza) (4.56.0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from stanza) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yhl125/opt/anaconda3/lib/python3.8/site-packages (from requests->stanza) (2020.12.5)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169294 sha256=b22a9330c879459994ae6518bb6112d9a0baf7ba40df4880fe8fc9552d5519aa\n",
      "  Stored in directory: /Users/yhl125/Library/Caches/pip/wheels/04/29/50/1e7189f03d2cf139e469863d54a1d3eabeb10c92c84e51f8a1\n",
      "Successfully built emoji\n",
      "Installing collected packages: torch, protobuf, emoji, stanza\n",
      "Successfully installed emoji-1.6.1 protobuf-3.19.1 stanza-1.3.0 torch-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wrong-nebraska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149397a7ffd24b0c92455d23f23c9447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 21:29:04 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bbf76d9f34477aa66c70596e073b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.3.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 21:37:22 INFO: Finished downloading models and saved to /Users/yhl125/stanza_resources.\n",
      "2021-11-14 21:37:22 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2021-11-14 21:37:22 INFO: Use device: cpu\n",
      "2021-11-14 21:37:22 INFO: Loading: tokenize\n",
      "2021-11-14 21:37:22 INFO: Loading: pos\n",
      "2021-11-14 21:37:22 INFO: Loading: lemma\n",
      "2021-11-14 21:37:22 INFO: Loading: depparse\n",
      "2021-11-14 21:37:23 INFO: Loading: sentiment\n",
      "2021-11-14 21:37:23 INFO: Loading: constituency\n",
      "2021-11-14 21:37:24 INFO: Loading: ner\n",
      "2021-11-14 21:37:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accepted-juvenile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 21:53:48 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2021-11-14 21:53:48 INFO: Use device: cpu\n",
      "2021-11-14 21:53:48 INFO: Loading: tokenize\n",
      "2021-11-14 21:53:48 INFO: Loading: pos\n",
      "2021-11-14 21:53:48 INFO: Loading: lemma\n",
      "2021-11-14 21:53:48 INFO: Loading: depparse\n",
      "2021-11-14 21:53:49 INFO: Loading: sentiment\n",
      "2021-11-14 21:53:49 INFO: Loading: constituency\n",
      "2021-11-14 21:53:50 INFO: Loading: ner\n",
      "2021-11-14 21:53:50 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stanza.models.common.doc.Document'>\n"
     ]
    }
   ],
   "source": [
    "en_nlp = stanza.Pipeline('en')\n",
    "en_doc = en_nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "print(type(en_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "balanced-river",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence 1]\n",
      "Barack      \tBarack      \tPROPN \t4\tnsubj:pass  \n",
      "Obama       \tObama       \tPROPN \t1\tflat        \n",
      "was         \tbe          \tAUX   \t4\taux:pass    \n",
      "born        \tbear        \tVERB  \t0\troot        \n",
      "in          \tin          \tADP   \t6\tcase        \n",
      "Hawaii      \tHawaii      \tPROPN \t4\tobl         \n",
      ".           \t.           \tPUNCT \t4\tpunct       \n",
      "\n",
      "[Sentence 2]\n",
      "He          \the          \tPRON  \t3\tnsubj:pass  \n",
      "was         \tbe          \tAUX   \t3\taux:pass    \n",
      "elected     \telect       \tVERB  \t0\troot        \n",
      "president   \tpresident   \tNOUN  \t3\txcomp       \n",
      "in          \tin          \tADP   \t6\tcase        \n",
      "2008        \t2008        \tNUM   \t3\tobl         \n",
      ".           \t.           \tPUNCT \t3\tpunct       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(en_doc.sentences):\n",
    "    print(\"[Sentence {}]\".format(i+1))\n",
    "    for word in sent.words:\n",
    "        print(\"{:12s}\\t{:12s}\\t{:6s}\\t{:d}\\t{:12s}\".format(\\\n",
    "              word.text, word.lemma, word.pos, word.head, word.deprel))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "under-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mention text\tType\tStart-End\n",
      "Barack Obama\tPERSON\t0-12\n",
      "Hawaii\tGPE\t25-31\n",
      "2008\tDATE\t62-66\n"
     ]
    }
   ],
   "source": [
    "print(\"Mention text\\tType\\tStart-End\")\n",
    "for ent in en_doc.ents:\n",
    "    print(\"{}\\t{}\\t{}-{}\".format(ent.text, ent.type, ent.start_char, ent.end_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lightweight-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 1,\n",
      "  \"text\": \"Barack\",\n",
      "  \"lemma\": \"Barack\",\n",
      "  \"upos\": \"PROPN\",\n",
      "  \"xpos\": \"NNP\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 4,\n",
      "  \"deprel\": \"nsubj:pass\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "word = en_doc.sentences[0].words[0]\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stuck-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 22:05:37 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-11-14 22:05:37 INFO: Use device: cpu\n",
      "2021-11-14 22:05:37 INFO: Loading: tokenize\n",
      "2021-11-14 22:05:37 INFO: Loading: ner\n",
      "2021-11-14 22:05:38 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Chris Manning\ttype: PERSON\n",
      "entity: Stanford University\ttype: ORG\n",
      "entity: the Bay Area\ttype: LOC\n",
      "You may leak 'name' information.\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "en_doc = en_nlp(\"I'm so happy because today is my son Bob's birthday. He is a student in University of Glasgow. We will have birthday party in Glasgow.\")\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n",
    "for ent in en_doc.ents:\n",
    "     if ent.type == 'PERSON':\n",
    "        print(\"You may leak 'name' information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "complete-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mention text\tType\tStart-End\n",
      "today\tDATE\t21-26\n",
      "Bob\tPERSON\t37-40\n",
      "You may leak 'name' information.\n",
      "University of Glasgow\tORG\t72-93\n",
      "Glasgow\tGPE\t126-133\n"
     ]
    }
   ],
   "source": [
    "print(\"Mention text\\tType\\tStart-End\")\n",
    "for ent in en_doc.ents:\n",
    "    print(\"{}\\t{}\\t{}-{}\".format(ent.text, ent.type, ent.start_char, ent.end_char))\n",
    "    if ent.type == 'PERSON':\n",
    "        print(\"You may leak 'name' information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unsigned-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-14 22:04:28 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-11-14 22:04:28 INFO: Use device: cpu\n",
      "2021-11-14 22:04:28 INFO: Loading: tokenize\n",
      "2021-11-14 22:04:28 INFO: Loading: ner\n",
      "2021-11-14 22:04:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Chris Manning\ttype: PERSON\n",
      "entity: Stanford University\ttype: ORG\n",
      "entity: the Bay Area\ttype: LOC\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "doc = nlp(\"Chris Manning teaches at Stanford University. He lives in the Bay Area.\")\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-giant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
