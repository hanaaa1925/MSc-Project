{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "current-wrestling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 15:16:48 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-12-04 15:16:48 INFO: Use device: cpu\n",
      "2021-12-04 15:16:48 INFO: Loading: tokenize\n",
      "2021-12-04 15:16:48 INFO: Loading: ner\n",
      "2021-12-04 15:16:49 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# data preprocess tools\n",
    "from nltk import data\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "import stanza\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "confused-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./dataset2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "pediatric-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].str.lower()\n",
    "data['content'] = data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('@')]))\n",
    "data['content'] = data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('#')]))\n",
    "data['content'] = data['content'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('http')]))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('rt')\n",
    "\n",
    "def text_process(text):\n",
    "    tokenizer = RegexpTokenizer('[a-z0-9]+')\n",
    "    token = tokenizer.tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token = [lemmatizer.lemmatize(w) for w in token if lemmatizer.lemmatize(w) not in stop_words]\n",
    "    return token\n",
    "\n",
    "data['content'] = data['content'].apply(text_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "purple-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:98.266%, precision:96.476%, recall:99.545%, dts1：98.27%\n",
      "accuracy:83.846%, precision:67.925%, recall:90.000%, dts2：83.85%\n"
     ]
    }
   ],
   "source": [
    "X = data['content']\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 118)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "non_sensitive_train = train[train['label'] == 0]\n",
    "sensitive_train = train[train['label'] == 1]\n",
    "\n",
    "non_sensitive_trainset = non_sensitive_train['content']\n",
    "sensitive_trainset = sensitive_train['content']\n",
    "\n",
    "vocablist = []\n",
    "\n",
    "for i in pd.concat([non_sensitive_trainset, sensitive_trainset]):\n",
    "    vocablist += i\n",
    "\n",
    "trainset_texts = [' '.join(content) for content in np.concatenate((non_sensitive_trainset.values, sensitive_trainset.values))]\n",
    "\n",
    "train_all_texts = [' '.join(content) for content in train['content']]\n",
    "\n",
    "test_all_texts = [' '. join(content) for content in test['content']]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "trainset_fit = cv.fit(trainset_texts)\n",
    "train_all_count = cv.transform(train_all_texts)\n",
    "test_all_count = cv.transform(test_all_texts)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf_matrix = tfidf.fit_transform(train_all_count)\n",
    "test_tfidf_matrix = tfidf.fit_transform(test_all_count)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "model = MultinomialNB(alpha=1.1, fit_prior=True, class_prior=None).fit(train_tfidf_matrix, y_train)\n",
    "#model.score(test_tfidf_matrix, y_test)\n",
    "\n",
    "y_train_pdt = model.predict(train_tfidf_matrix)\n",
    "y_test_pdt = model.predict(test_tfidf_matrix)\n",
    "\n",
    "dts1 = len(np.where(y_train_pdt==y_train)[0])/len(y_train)\n",
    "dts2 = len(np.where(y_test_pdt==y_test)[0])/len(y_test)\n",
    "\n",
    "acc1 = accuracy_score(y_train_pdt, y_train)\n",
    "acc2 = accuracy_score(y_test_pdt, y_test)\n",
    "\n",
    "pre1 = precision_score(y_train_pdt, y_train)\n",
    "pre2 = precision_score(y_test_pdt, y_test)\n",
    "\n",
    "rec1 = recall_score(y_train_pdt, y_train)\n",
    "rec2 = recall_score(y_test_pdt, y_test)\n",
    "\n",
    "print(\"accuracy:{:.3f}%, precision:{:.3f}%, recall:{:.3f}%, dts1：{:.2f}%\".format(acc1*100, pre1*100, rec1*100, dts1*100))\n",
    "print(\"accuracy:{:.3f}%, precision:{:.3f}%, recall:{:.3f}%, dts2：{:.2f}%\".format(acc2*100, pre2*100, rec2*100, dts2*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text = \"@Alice My name is Bob, Today is my Birthday. My salary is $2000 per month, I work in Microsoft.My workmate is bad\"\n",
    "#text = \"Today is my Birthday.\"\n",
    "#text = \"My salary is $2000 per month\"\n",
    "#text = \"My boss Roan is a foolish man\"\n",
    "#text = \"I hope Alice get fired tommorrow\"\n",
    "#text = \"My workmate is nice\"\n",
    "#text = \"My workmate is bad\"\n",
    "#text = \"My workmate is shameless\"\n",
    "#text = \"My employee John is good\"\n",
    "#text = \"My employee John is good but mean\"\n",
    "text = \"My employee John is asshole, but he pays me a lot\"\n",
    "#text = \"my salary is $200 pounds every month\"\n",
    "\n",
    "#df = pd.read_excel('./test.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"text\":text},index=[\"0\"])\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('@')]))\n",
    "df['text'] = df['text'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('#')]))\n",
    "df['text'] = df['text'].apply(lambda string: ' '.join([word for word in string.split(' ') if not word.rstrip(' ').startswith('http')]))\n",
    "df['text'] = df['text'].apply(text_process)\n",
    "#print(df['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'])\n",
    "#y_label = df['label']\n",
    "text_content = [' '.join(content) for content in df['text']]\n",
    "text_count = cv.transform(text_content)\n",
    "text_tfidf_matrix = tfidf.fit_transform(text_count)\n",
    "                \n",
    "pre = model.predict(text_tfidf_matrix)\n",
    "print(pre)\n",
    "#acc_test = accuracy_score(pre, y_label)\n",
    "#print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = model.predict_proba(test_tfidf_matrix)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:,1])\n",
    "roc_auc = metrics.auc(fpr,tpr)\n",
    "\n",
    "plt.figure(figsize=(5,5), dpi=100)\n",
    "plt.plot(fpr, tpr, 'b', label = 'Val AUC = %0.3f' % roc_auc)\n",
    "plt.title('roc = {:.4f}'.format(roc_auc))\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-second",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature of engineering\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "cv = CountVectorizer()\n",
    "trainset_fit = cv.fit(trainset_texts)\n",
    "train_all_count = cv.fit_transform(train_all_texts)\n",
    "test_all_count = cv.transform(test_all_texts)\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "train_tfidf_matrix = sel.fit_transform(train_all_count)\n",
    "test_tfidf_matrix = sel.fit_transform(test_all_count)\n",
    "\n",
    "#test_sel\n",
    "#\n",
    "#tfidf = TfidfTransformer()\n",
    "#train_tfidf_matrix = tfidf.fit_transform(train_sel)\n",
    "#test_tfidf_matrix = tfidf.fit_transform(test_sel)\n",
    "\n",
    "test_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "newData = pca.fit_transform(train_all_count)\n",
    "print (newData)\n",
    "\n",
    "pre = clf.predict(X)\n",
    "Y = [1,1,0,0,1,0,0,1,1,0]\n",
    "import matplotlib.pyplot as plt\n",
    "L1 = [n[0] for n in newData]\n",
    "L2 = [n[1] for n in newData]\n",
    "plt.scatter(L1,L2,c=pre,s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词袋\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "\n",
    "train_word_list = []\n",
    "for corpu in train_all_texts:\n",
    "    seg_list = [i for i in corpu] \n",
    "    word_list.append(seg_list)\n",
    "\n",
    "#print(word_list)\n",
    "\n",
    "test_word_list = []\n",
    "for corpu in test_all_texts:\n",
    "    seg_list = [i for i in corpu] \n",
    "    test_word_list.append(seg_list)\n",
    "\n",
    "    \n",
    "train_dictionary = corpora.Dictionary(train_word_list)\n",
    "test_dictionary = corpora.Dictionary(test_word_list)\n",
    "\n",
    "#print(dictionary.token2id)\n",
    "trainset = [dictionary.doc2bow(word) for word in train_word_list]\n",
    "testset = [dictionary.doc2bow(word) for word in test_word_list]\n",
    "\n",
    "train_tfidf = models.TfidfModel(trainset)\n",
    "test_tfidf = models.TfidfModel(testset)\n",
    "\n",
    "train_tfidf_vec = []\n",
    "for i in trainset:\n",
    "    string_tfidf = train_tfidf[i]\n",
    "    train_tfidf_vec.append(string_tfidf)\n",
    "print(train_tfidf_vec)\n",
    "\n",
    "\n",
    "test_tfidf_vec = []\n",
    "for i in testset:\n",
    "    string_tfidf = test_tfidf[i]\n",
    "    test_tfidf_vec.append(string_tfidf)\n",
    "#print(test_tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-minister",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
